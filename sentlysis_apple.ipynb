{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rizarumayantidewi/Advanced-Data-Mining/blob/main/sentlysis_apple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEHJCo_CUmHU"
      },
      "source": [
        "**Name:** Riza Rumayanti Dewi\n",
        "\n",
        "**Student ID:** 20240130015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzxxGKrnVYVK"
      },
      "source": [
        "# Sentiment Analysis Project\n",
        "\n",
        "## CRISP-DM\n",
        "\n",
        "\n",
        "reference:\n",
        "https://andiyudha.medium.com/crisp-dm-pendekatan-proses-dalam-data-mining-68bf8c2dc908\n",
        "\n",
        "https://github.com/rainavyas/IMDB_Sentiment_Classification.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2fFuDoJWTwL"
      },
      "source": [
        "# Businesss/Research Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0M2b44uYulL"
      },
      "source": [
        "build a deep learning machine learning model to predict sentiment (positive, neutral, or negative) from user reviews of Apple products scraped from Twitter\n",
        "\n",
        "Terms:\n",
        "(bellow : use BERT, ELECTRA, and ROBERTA as the main algorithms.)\n",
        "perform hyperparameter tuning for each model.\n",
        "evaluate model performance using relevant metrics, such as accuracu and F1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8-Evu2SWZZ4"
      },
      "source": [
        "# Data Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRUKcV3maCB"
      },
      "source": [
        "Training data,\n",
        "if i used for naive bayes, LogisticRegression, RandomForestClassifier, SVC, Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqNRgBLcb5vR"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "lV2FS7uUzrAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0ade95-3ada-4f59-9b88-a21bbf8e445a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "9-wEoizCzvY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "3ef98cea-5f5a-4214-f361-39a701f7265d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4757 entries, 0 to 4756\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   labels  4757 non-null   float64\n",
            " 1   tweets  4757 non-null   object \n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 74.5+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "labels    0\n",
              "tweets    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>labels</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweets</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "# Load the uploaded file\n",
        "file_path = \"train.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic info and first few rows\n",
        "df.info(), df.head()\n",
        "\n",
        "data = pd.read_csv(\"train.csv\", encoding='latin1')\n",
        "data = data[['labels', 'tweets']]\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = data.copy()\n",
        "new_data['labels'] = new_data['labels'].map({'negative': -1, 'neutral': 0, 'positive': 1})\n",
        "new_data.loc[new_data['labels'] == 'not_relevant', 'labels'] = 4\n",
        "\n",
        "data['labels'].replace({'not_relevant': 4}, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eAgVXMeA5-t",
        "outputId": "008f30b8-ea3a-4683-a8b1-ebf17aec21f7"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-127-c73b1e9c0165>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['labels'].replace({'not_relevant': 4}, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "l4pcs3bw1ZOk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "4360ec41-3611-4e43-e5b4-7a657ef174f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-128-d2587ea98a98>:1: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(x=\"labels\",data=data, palette=\"Set1\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='labels', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 128
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(x=\"labels\",data=data, palette=\"Set1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
        "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    tweet = re.sub(r'\\W', ' ', tweet)\n",
        "    tweet = re.sub(r'\\d+', '', tweet)\n",
        "    tweet = re.sub(r'\\S+@\\S+', '', tweet)\n",
        "    tokens = word_tokenize(tweet)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    preprocessed_tweet = ' '.join(tokens)\n",
        "    return preprocessed_tweet\n",
        "\n",
        "preprocessed_tweets = [preprocess_tweet(tweet) for tweet in data['tweets']]\n",
        "data['tweets'] = preprocessed_tweets\n",
        "data['tweets'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "Ruec32ybuICZ",
        "outputId": "0759ea2f-eac5-49cf-972d-d19676b8a334"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0            certified partnered business collaborated\n",
              "1                        tbh annoyed apple shit moment\n",
              "2    new product innovative service lead barclays h...\n",
              "3    patented iphone drop protection mechanism end ...\n",
              "4    rt customer service success amp fails amp exam...\n",
              "Name: tweets, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>certified partnered business collaborated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tbh annoyed apple shit moment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>new product innovative service lead barclays h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>patented iphone drop protection mechanism end ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rt customer service success amp fails amp exam...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "20k1qDG92LUG"
      },
      "outputs": [],
      "source": [
        "# Replace NaN values in the 'labels' column with a suitable integer\n",
        "data['labels'] = data['labels'].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "dCPWU2I02LFx"
      },
      "outputs": [],
      "source": [
        "X = data['tweets']\n",
        "y = data['labels']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "6pA0uToc2ciG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
        "X_test_counts = count_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "HFubcyjw2fry"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "classifiers = {\n",
        "    'Naïve Bayes': MultinomialNB(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Perceptron': Perceptron()\n",
        "}\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "classifiers['Logistic Regression'] = log_reg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.value_counts())\n",
        "print(y_test.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBg7c1IY_yNO",
        "outputId": "e7fd396e-435a-4cb2-fecd-b57f89e8961a"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels\n",
            " 0    2075\n",
            "-1    1322\n",
            " 1     408\n",
            "Name: count, dtype: int64\n",
            "labels\n",
            " 0    495\n",
            "-1    340\n",
            " 1    117\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "jBwExqvP4pej"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "results = {}\n",
        "averages=['macro','micro']\n",
        "for avg in averages:\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        clf.fit(X_train_counts, y_train)\n",
        "        y_pred = clf.predict(X_test_counts)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average=avg,zero_division=1)\n",
        "        recall = recall_score(y_test, y_pred, average=avg)\n",
        "        f1 = f1_score(y_test, y_pred, average=avg)\n",
        "        results[(clf_name,avg)]= {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "mB-PkfH46ELB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "71dc6fa4-affa-4e9f-e42f-d07cb129e0f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           Accuracy  Precision    Recall        F1  \\\n",
              "Naïve Bayes         macro  0.771008   0.767712  0.682180  0.704741   \n",
              "Logistic Regression macro  0.810924   0.839563  0.722464  0.758551   \n",
              "Random Forest       macro  0.801471   0.820209  0.705886  0.740683   \n",
              "SVM                 macro  0.784664   0.833221  0.670900  0.709444   \n",
              "Perceptron          macro  0.792017   0.759375  0.736090  0.746452   \n",
              "Naïve Bayes         micro  0.771008   0.771008  0.771008  0.771008   \n",
              "Logistic Regression micro  0.810924   0.810924  0.810924  0.810924   \n",
              "Random Forest       micro  0.813025   0.813025  0.813025  0.813025   \n",
              "SVM                 micro  0.784664   0.784664  0.784664  0.784664   \n",
              "Perceptron          micro  0.792017   0.792017  0.792017  0.792017   \n",
              "\n",
              "                               Feature Extraction Technique  \n",
              "Naïve Bayes         macro  Bag of words based on raw counts  \n",
              "Logistic Regression macro  Bag of words based on raw counts  \n",
              "Random Forest       macro  Bag of words based on raw counts  \n",
              "SVM                 macro  Bag of words based on raw counts  \n",
              "Perceptron          macro  Bag of words based on raw counts  \n",
              "Naïve Bayes         micro  Bag of words based on raw counts  \n",
              "Logistic Regression micro  Bag of words based on raw counts  \n",
              "Random Forest       micro  Bag of words based on raw counts  \n",
              "SVM                 micro  Bag of words based on raw counts  \n",
              "Perceptron          micro  Bag of words based on raw counts  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77b08922-aec8-4b81-ad0e-618760573f87\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Feature Extraction Technique</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Naïve Bayes</th>\n",
              "      <th>macro</th>\n",
              "      <td>0.771008</td>\n",
              "      <td>0.767712</td>\n",
              "      <td>0.682180</td>\n",
              "      <td>0.704741</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Logistic Regression</th>\n",
              "      <th>macro</th>\n",
              "      <td>0.810924</td>\n",
              "      <td>0.839563</td>\n",
              "      <td>0.722464</td>\n",
              "      <td>0.758551</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <th>macro</th>\n",
              "      <td>0.801471</td>\n",
              "      <td>0.820209</td>\n",
              "      <td>0.705886</td>\n",
              "      <td>0.740683</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <th>macro</th>\n",
              "      <td>0.784664</td>\n",
              "      <td>0.833221</td>\n",
              "      <td>0.670900</td>\n",
              "      <td>0.709444</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Perceptron</th>\n",
              "      <th>macro</th>\n",
              "      <td>0.792017</td>\n",
              "      <td>0.759375</td>\n",
              "      <td>0.736090</td>\n",
              "      <td>0.746452</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Naïve Bayes</th>\n",
              "      <th>micro</th>\n",
              "      <td>0.771008</td>\n",
              "      <td>0.771008</td>\n",
              "      <td>0.771008</td>\n",
              "      <td>0.771008</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Logistic Regression</th>\n",
              "      <th>micro</th>\n",
              "      <td>0.810924</td>\n",
              "      <td>0.810924</td>\n",
              "      <td>0.810924</td>\n",
              "      <td>0.810924</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <th>micro</th>\n",
              "      <td>0.813025</td>\n",
              "      <td>0.813025</td>\n",
              "      <td>0.813025</td>\n",
              "      <td>0.813025</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <th>micro</th>\n",
              "      <td>0.784664</td>\n",
              "      <td>0.784664</td>\n",
              "      <td>0.784664</td>\n",
              "      <td>0.784664</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Perceptron</th>\n",
              "      <th>micro</th>\n",
              "      <td>0.792017</td>\n",
              "      <td>0.792017</td>\n",
              "      <td>0.792017</td>\n",
              "      <td>0.792017</td>\n",
              "      <td>Bag of words based on raw counts</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77b08922-aec8-4b81-ad0e-618760573f87')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-77b08922-aec8-4b81-ad0e-618760573f87 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-77b08922-aec8-4b81-ad0e-618760573f87');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dee97814-56ca-4fea-8c22-64fb1a52acc6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dee97814-56ca-4fea-8c22-64fb1a52acc6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dee97814-56ca-4fea-8c22-64fb1a52acc6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f63cec02-ec88-4d78-8057-be321697adff\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f63cec02-ec88-4d78-8057-be321697adff button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.015716959907671035,\n        \"min\": 0.7710084033613446,\n        \"max\": 0.8130252100840336,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.7710084033613446,\n          0.8109243697478992,\n          0.8130252100840336\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02826804112526979,\n        \"min\": 0.7593746504637594,\n        \"max\": 0.8395630557558279,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7846638655462185,\n          0.8395630557558279,\n          0.7710084033613446\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05253005928645599,\n        \"min\": 0.6708997973703856,\n        \"max\": 0.8130252100840336,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7846638655462185,\n          0.7224644636409342,\n          0.7710084033613446\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03833835297024872,\n        \"min\": 0.7047410294785594,\n        \"max\": 0.8130252100840336,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7846638655462185,\n          0.7585510254982282,\n          0.7710084033613446\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Feature Extraction Technique\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Bag of words based on raw counts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "# I am Transposing the DataFrame to have classifiers as rows and metrics as columns\n",
        "results_df = results_df.T\n",
        "results_df['Feature Extraction Technique']='Bag of words based on raw counts'\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT\n",
        "\n",
        "test data implementation"
      ],
      "metadata": {
        "id": "EUNHJgV-u2C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "-DCipc_81gTu"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"test.csv\", encoding='latin1')\n",
        "df = df[['tweets']]\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
        "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    tweet = re.sub(r'\\W', ' ', tweet)\n",
        "    tweet = re.sub(r'\\d+', '', tweet)\n",
        "    return tweet\n",
        "\n",
        "df['tweets'] = df['tweets'].apply(preprocess_tweet)\n"
      ],
      "metadata": {
        "id": "10N-sptX1l8L"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test = train_test_split(df['tweets'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "6JOjA6qK2GzA"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)"
      ],
      "metadata": {
        "id": "ARmOLMUg2hi3"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors\n",
        "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
        "test_labels = torch.tensor(y_test[:len(X_test)].values)\n",
        "# Create DataLoader\n",
        "test_data = TensorDataset(test_inputs, test_labels)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "nQUInpc82uhb"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT Model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tZ6fPjI3BF-",
        "outputId": "c61c0c12-1ddc-4efe-abb4-7eb3fe69f2db"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Assuming your original labels are -1, 0, 1\n",
        "label_mapping = {-1: 0, 0: 1, 1: 2}  # Map -1 to 0, 0 to 1, 1 to 2\n",
        "\n",
        "# Load Dataset\n",
        "# ... (your code to load train.csv and test.csv) ...\n",
        "\n",
        "# Training Loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        inputs, labels = batch  # Assuming your batch contains (input_ids, labels)\n",
        "\n",
        "        # Map labels before moving to device\n",
        "        labels = torch.tensor([label_mapping[label.item()] for label in labels])\n",
        "\n",
        "        # Move tensors to the device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Call the model with correct arguments\n",
        "        outputs = model(inputs, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFfTY5HF3HKM",
        "outputId": "4460bfce-c258-4409-e68f-2a1665449020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 15/15 [01:38<00:00,  6.57s/it, loss=1.09]\n",
            "Epoch 1: 100%|██████████| 15/15 [01:31<00:00,  6.10s/it, loss=1.14]\n",
            "Epoch 2: 100%|██████████| 15/15 [01:34<00:00,  6.33s/it, loss=1.12]\n",
            "Epoch 3:  60%|██████    | 9/15 [00:55<00:36,  6.15s/it, loss=1.01]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "preds = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        preds.extend(predictions.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(true_labels, preds))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, preds))"
      ],
      "metadata": {
        "id": "Q2f27Z6I4uS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **my draft to try other algorithms**"
      ],
      "metadata": {
        "id": "tBBCcimM55cP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wosC3MQhmd5J"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch scikit-learn nltk tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtVqU9yvmjSg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import transformers\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
        "\n",
        "# Download stopwords and punkt_tab\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Add this line to download the required data\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Gunakan GPU jika tersedia\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YXbA3samwrE"
      },
      "outputs": [],
      "source": [
        "def load_data(train_path, test_path):\n",
        "    train_df = pd.read_csv(train_path, header=None, names=['tweets', 'labels'])  # Set header to None\n",
        "    test_df = pd.read_csv(test_path, header=None, names=['tweets', 'labels'])  # Set header to None\n",
        "    return train_df, test_df\n",
        "\n",
        "train_path = \"train.csv\"\n",
        "test_path = \"test.csv\"\n",
        "\n",
        "train_df, test_df = load_data(train_path, test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF9tyM0omx7Q"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Check if text is a string, handle float (e.g., NaN)\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Hapus URL\n",
        "        text = re.sub(r'\\@\\w+|\\#','', text)  # Hapus mention dan hashtag\n",
        "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)  # Hapus karakter spesial\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [word for word in tokens if word not in stop_words]  # Hapus stopwords\n",
        "        return \" \".join(tokens)\n",
        "    else:  # If not a string (e.g., float), return an empty string or handle as needed\n",
        "        # Return a placeholder string instead of an empty string\n",
        "        return \"placeholder\"\n",
        "\n",
        "# Terapkan preprocessing\n",
        "# Access the correct column in train_df (column with index 0)\n",
        "train_df[train_df.columns[0]] = train_df[train_df.columns[0]].apply(clean_text)\n",
        "\n",
        "# Assuming 'labels' is the second column in train_df:\n",
        "train_df['labels'] = train_df[train_df.columns[1]].map({-1: 0, 0: 1, 1: 2}) # Change to create a 'labels' column\n",
        "\n",
        "# Access the correct column in test_df ('tweets')\n",
        "test_df['tweets'] = test_df['tweets'].apply(clean_text)\n",
        "\n",
        "# Assuming 'labels' is the second column in test_df as well:\n",
        "test_df['labels'] = test_df[test_df.columns[0]].map({-1: 0, 0: 1, 1: 2}) # Change to create a 'labels' column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmtQclsWm1pN"
      },
      "outputs": [],
      "source": [
        "# Pilih model yang ingin digunakan: BERT, ELECTRA, atau RoBERTa\n",
        "model_name = \"bert-base-uncased\"  # Bisa diganti dengan \"google/electra-small-discriminator\" atau \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"tweet\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Tokenisasi dataset\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Konversi dataset ke PyTorch DataLoader\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"labels\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"labels\"])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(train_dataset.features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-JazUz1uTc0"
      },
      "outputs": [],
      "source": [
        "# Pilih model yang ingin digunakan: BERT, ELECTRA, atau RoBERTa\n",
        "model_name = \"roberta-base\"  # Bisa diganti dengan \"google/electra-small-discriminator\" atau \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"tweet\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Tokenisasi dataset\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Konversi dataset ke PyTorch DataLoader\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"labels\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"labels\"])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(train_dataset.features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhmau1N-udbS"
      },
      "outputs": [],
      "source": [
        "# Pilih model yang ingin digunakan: BERT, ELECTRA, atau RoBERTa\n",
        "model_name = \"google/electra-small-discriminator\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"tweet\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Tokenisasi dataset\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Konversi dataset ke PyTorch DataLoader\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"labels\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"labels\"])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(train_dataset.features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD_DEb1lm9b6"
      },
      "outputs": [],
      "source": [
        "# Load Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer & Loss Function\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Scheduler\n",
        "num_training_steps = len(train_loader) * 3\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_batches = 0  # Initialize a counter for the number of batches\n",
        "    for batch in tqdm(train_loader):\n",
        "        inputs = {key: batch[key].to(device) for key in [\"input_ids\", \"attention_mask\"]}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1  # Increment the batch counter\n",
        "\n",
        "    # Check if num_batches is 0 before dividing\n",
        "    if num_batches > 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss = {running_loss / num_batches:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1}: Loss = No batches in train_loader\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzc8xVgrvdLh"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset columns:\", test_dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wf7i8YNvXlW"
      },
      "outputs": [],
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    print(\"Processing batch...\")  # Debugging\n",
        "    inputs = {key: val.to(device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\"]}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    all_preds.extend(predictions.cpu().numpy())\n",
        "    all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "print(\"Total Predictions:\", len(all_preds))\n",
        "print(\"Total Labels:\", len(all_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXNJJ_EVvSaz"
      },
      "outputs": [],
      "source": [
        "print(\"Total samples in test_loader:\", len(test_loader.dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHrliHfDnG_3"
      },
      "outputs": [],
      "source": [
        "# Evaluasi model\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        inputs = {key: batch[key].to(device) for key in [\"input_ids\", \"attention_mask\"]}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Hasil evaluasi\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxg4_WxsnGwf"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"sentiment_model\")\n",
        "tokenizer.save_pretrained(\"sentiment_model\")\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eNDoV9zRea75"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTwS9mBB0v4Q"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwLm2gEylOgw"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9dqzveEb2jR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load Dataset\n",
        "def load_data(train_path, test_path):\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    return train_df, test_df\n",
        "\n",
        "train_path = \"train.csv\"\n",
        "test_path = \"test.csv\"\n",
        "\n",
        "train_df, test_df = load_data(train_path, test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgllPCGee-iR"
      },
      "outputs": [],
      "source": [
        "# Preprocessing Data\n",
        "def preprocess_data(df):\n",
        "    df = df[['tweets', 'labels']].dropna()\n",
        "    df['labels'] = df['labels'].map({-1: 0, 0: 1, 1: 2})  # Re-map labels for compatibility\n",
        "    return df\n",
        "\n",
        "train_df = pd.read_csv(train_path, header=None, names=['tweet', 'labels'])\n",
        "test_df = pd.read_csv(test_path, header=None, names=['tweet', 'labels'])\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "print(train_df.head())\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nuhWaAnK1ZS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "data1 = pd.read_csv(\"train.csv\", header=None, names=['tweet', 'labels'])\n",
        "data2 = pd.read_csv(\"test.csv\", header=None, names=['tweet', 'labels'])\n",
        "\n",
        "# Remap labels to sentiment categories\n",
        "sentiment_mapping = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "data1['sentiment'] = data1['labels'].map(sentiment_mapping)\n",
        "data2['sentiment'] = data2['labels'].map(sentiment_mapping)\n",
        "\n",
        "# Calculate sentiment counts\n",
        "sentiment_counts1 = data1['sentiment'].value_counts().sort_index()\n",
        "sentiment_counts2 = data2['sentiment'].value_counts().sort_index()\n",
        "\n",
        "# Create the bar plot for data1 (train data)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=sentiment_counts1.index, y=sentiment_counts1.values, palette=\"pastel\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count of Tweets\")\n",
        "plt.title(\"Train Data: Tweet Sentiment Distribution\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Annotate the bars with their values\n",
        "for i, v in enumerate(sentiment_counts1.values):\n",
        "    plt.text(i, v + 10, str(v), ha='center')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Create the bar plot for data2 (test data)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=sentiment_counts2.index, y=sentiment_counts2.values, palette=\"pastel\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count of Tweets\")\n",
        "plt.title(\"Test Data: Tweet Sentiment Distribution\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Annotate the bars with their values\n",
        "for i, v in enumerate(sentiment_counts2.values):\n",
        "    plt.text(i, v + 10, str(v), ha='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ItRneDrK1VS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load data (ganti dengan path file Anda)\n",
        "df = pd.read_csv(\"data_ulasan.csv\")\n",
        "\n",
        "# Konversi kolom tanggal ke format datetime\n",
        "df['tanggal'] = pd.to_datetime(df['tanggal'])\n",
        "\n",
        "# Kelompokkan data berdasarkan minggu dan hitung jumlah data per minggu\n",
        "weekly_data = df.groupby(pd.Grouper(key='tanggal', freq='W')).size().reset_index(name='jumlah_data')\n",
        "\n",
        "# Visualisasi jumlah data per minggu\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='tanggal', y='jumlah_data', data=weekly_data, palette='pastel')\n",
        "plt.xlabel('Minggu')\n",
        "plt.ylabel('Jumlah Data')\n",
        "plt.title('Jumlah Data per Minggu')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# Preprocessing teks untuk menghitung frekuensi kata\n",
        "def preprocess_text(text):\n",
        "    # Tokenisasi\n",
        "    words = word_tokenize(text)\n",
        "    # Hapus stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Tokenisasi dan hitung frekuensi kata\n",
        "all_words = []\n",
        "for ulasan in df['ulasan']:\n",
        "    words = preprocess_text(ulasan)\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Hitung frekuensi kata\n",
        "word_freq = pd.Series(all_words).value_counts()[:15]\n",
        "\n",
        "# Visualisasi frekuensi kata\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=word_freq.index, y=word_freq.values, palette='pastel')\n",
        "plt.xlabel('Kata')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.title('Frekuensi Kata Terbanyak')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f90vsbGoNMOC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = {'Week': ['11-Nov-24', '18-Nov-24', '25-Nov-24', '02-Dec-24', '09-Dec-24', '16-Dec-24'],\n",
        "        'Data Count': [500, 800, 550, 600, 1500, 2000]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='Week', y='Data Count', data=df, palette='pastel')\n",
        "plt.title('Number of Data Per Week')\n",
        "plt.xlabel('Weeks')\n",
        "plt.ylabel('Data Count')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Annotate the bars with their values\n",
        "for i, v in enumerate(df['Data Count']):\n",
        "    plt.text(i, v + 10, str(v), ha='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YROmkOABNNYM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample data (replace with your actual word frequency data)\n",
        "data = {'Word': ['baik', 'toko', 'belanja', 'produk', 'cepat', 'mudah', 'terima', 'kualitas', 'barang', 'puas', 'cepat', 'respon', 'toko', 'terima', 'kasih'],\n",
        "        'Word Count': [2000, 1500, 1200, 1100, 1000, 900, 800, 750, 700, 650, 600, 550, 500, 450, 400]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.barplot(x='Word', y='Word Count', data=df, palette='pastel')\n",
        "plt.title('Word Frequency')\n",
        "plt.xlabel('15 Frequently Occurring Words')\n",
        "plt.ylabel('Word Count')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Annotate the bars with their values\n",
        "for i, v in enumerate(df['Word Count']):\n",
        "    plt.text(i, v + 10, str(v), ha='center')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsy1rqbhWeLb"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY3TGWqbl_ZH"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import nltk\n",
        "\n",
        "# Hapus folder nltk_data\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True)\n",
        "\n",
        "# Unduh ulang stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxXGPutw2h6p"
      },
      "outputs": [],
      "source": [
        "print(train_df.columns)  # Check column names in train_df\n",
        "print(test_df.columns)   # Check column names in test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LydhwEAb4bzs"
      },
      "outputs": [],
      "source": [
        "train_df.columns = train_df.columns.str.strip()\n",
        "test_df.columns = test_df.columns.str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7ENtbVw2tiZ"
      },
      "outputs": [],
      "source": [
        "# Lihat beberapa baris pertama dari dataset untuk memverifikasi kolom\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81AaFiN0oNcf"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocessing Data\n",
        "def preprocess_data(df):\n",
        "    df = df[['tweets', 'labels']].dropna()\n",
        "    df['labels'] = df['labels'].map({-1: 0, 0: 1, 1: 2})  # Re-map labels for compatibility\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_eHpLFq7CSw"
      },
      "outputs": [],
      "source": [
        "print(train_df.head())  # Menampilkan beberapa baris pertama dari dataset train_df\n",
        "print(test_df.head())   # Menampilkan beberapa baris pertama dari dataset test_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNfboX8GWhkW"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdtWssnndpdI"
      },
      "source": [
        "ELECTRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TwsnhlFdwPz"
      },
      "outputs": [],
      "source": [
        "model_name1 = \"google/electra-small-discriminator\"  # ELECTRA model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name1)\n",
        "model_electra = AutoModelForSequenceClassification.from_pretrained(model_name1, num_labels=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa0L6re1dr22"
      },
      "source": [
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gGFvymYd0mc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"bert-base-uncased\"  # BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPLnM6S6dtEd"
      },
      "source": [
        "ROBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEkAGBNKd3Cr"
      },
      "outputs": [],
      "source": [
        "model_name2 = \"roberta-base\"  # RoBERTa model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name2)\n",
        "model_roberta = AutoModelForSequenceClassification.from_pretrained(model_name2, num_labels=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDRn1ZdqCRuZ"
      },
      "source": [
        "Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgQCVRwHDDyi"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQUJB86dCX3K"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Load Dataset (pastikan data sudah siap)\n",
        "train_df, test_df = load_data(train_path, test_path)\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['tweets'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define Optuna Objective Function\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-4)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "    num_train_epochs = trial.suggest_int('num_train_epochs', 3, 5)\n",
        "\n",
        "    # Model and Trainer\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "    # Set training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    eval_result = trainer.evaluate()\n",
        "    return eval_result['eval_loss']\n",
        "\n",
        "# Create Optuna study and optimize the hyperparameters\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best hyperparameters: {study.best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ8ZIAqwYfmc"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yni1FCEPd6YC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluate predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "pred_labels = predictions.predictions.argmax(axis=1)\n",
        "true_labels = test_dataset[\"labels\"]\n",
        "\n",
        "print(classification_report(true_labels, pred_labels, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-76xvZjlYiYx"
      },
      "source": [
        "# Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvghn1pjT4EZ"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"sentiment_model\")\n",
        "tokenizer.save_pretrained(\"sentiment_model\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN67i/CcTfBeMg/ciJ91wij",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}